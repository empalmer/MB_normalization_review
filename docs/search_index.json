[["index.html", "Tutorial on Normalization of Microbiome Data Chapter 1 Introduction 1.1 The importance of normalization 1.2 The compositional nature of microbiome data", " Tutorial on Normalization of Microbiome Data Chapter 1 Introduction 1.1 The importance of normalization Microbiome data must be normalized before any statistical analysis can be performed. Following the process of sequencing and assigning raw reads into counts per observed and classified identified taxa classes/OTUs/ASVs, microbiome data consist of a matrix of read counts, referred to as a feature table of raw counts. Normalization is the process of transforming raw read count data into data that can be compared between samples. Statistical analysis on this count matrix is then performed depending on the goal of the experiment. Common analysis goals include community-level analysis (alpha/beta diversity), differential abundance testing (the parallel of differential expression testing in gene expression studies), and network analysis. Analysis of composition, differences, connections, etc., should be done based only on true biological aspects. However, technical variation in counts across samples is a given hurdle that must be accounted for. Biases can arise in the sequencing process, sample preparation, contamination, preferential amplification, and can manifest in differences in sparsity and unequal sequencing depths (Salter et al. 2014). An effective normalization strategy should put all samples on equal footing so interpretations are on biological signals, not technical signals such as sequencing depth. Currently, there is no known ‘best’ normalization method that removes all technical artifacts leaving only biological signals. Due to the sequencing technology, samples will have different sequencing depths, or the sum of all the counts in a sample. Directly comparing raw counts between samples is not possible. To illustrate this, consider the counts of Taxon 1, across two samples shown below. In Sample A, this taxon has a count of 230, and in Sample B, this taxon has a count of 23. Is this taxon differentially abundant between samples? As we see below, the way we normalize the data can change how we would answer this question. Table 1.1: The raw counts of taxon 1 are different, and the raw counts of taxon 2 are equal. Sampling depth is not accounted for. Taxa Sample A Sample B Taxon 1 230 23 Taxon 2 5 5 One goal for normalization of microbiome data is the standardization of sequencing depth across samples. One common approach to this is a scaling-based approach, where a scaling factor is calculated for every sample and the counts for each taxon are divided by the scaling factor for that sample. Figure B shows the same data as Figure A, but where each sample has been transformed into proportions by dividing by the total counts for each sample. The difference in Taxon 1 between samples appears much smaller. However, now there appears to be a difference in Taxon 2, even though the counts were originally the same. This is because in sample B, Taxon 1 consists of a higher proportion of the total count than in sample A. This demonstrates the importance of normalization, but also the artifacts that can occur depending on the method. 1.2 The compositional nature of microbiome data Microbiome data are inherently compositional. The counts of the collection of taxa that make up each sample are constrained by the total sum, or sequencing depth for that sample. This means that the count of each sampled taxon is a portion of a larger whole. Each observed taxon is not independent. As we saw in the above example, before normalization, Taxon 2 was equal between samples. After converting to proportions, Taxon 2 no longer appears equal. If there is a difference between two samples it is unclear if that difference is because of a true difference in that taxon or if that taxon is changing because of differences in another taxon. Numerous traditional statistical methods rely on an independence assumption, which is not met with microbiome data. This can lead to spurious correlations that exist only because of the compositional nature and not any true signal. With library size as the sum constraint for each sample, if we know in a biological system that after an event occurs (treatment), the red taxon decreases, this will change the composition of the sampled blue taxon regardless of its change or lack thereof in the underlying population. Consider again two samples consisting of red and blue points. We can think of the samples as before and after treatment. In the second plot, the number of red dots in the population and in the observed sample has decreased, but the blue remains the same. Table 1.2: Counts of sampled red and blue taxa before and after Sample blue red Before 10 20 After 10 4 Table 1.2: Proportions of sampled red and blue taxa before and after Sample blue red Before 0.2857143 0.6666667 After 0.7142857 0.3333333 This observed increase in the proportion of blue is due to the compositional nature of the sampled points, and not any true difference in the blue population. References "],["importing-data.html", "Chapter 2 Importing Data 2.1 Global Patterns 2.2 Colorectal cancer 2.3 Pre-processing Quality Control and Filtering", " Chapter 2 Importing Data There are multiple publicly available pre-compiled microbiome data sets of feature tables. These data sets begin after the bioinformatics pipeline and are matrices of counts of OTUs per sample. These data sets can exist as phyloseq objects, a popular R package for microbiome analysis (McMurdie and Holmes 2013), or as separate tables of feature counts and metadata. For this tutorial, we will provide code that assumes a phyloseq object as input, and outputs a normalized phyloseq object. Converting between separate tables and phyloseq objects is straightforward using the otu_table(), sample_data(), taxa_table(), and phyloseq() functions. For more information on the phyloseq package, see https://joey711.github.io/phyloseq/. In this tutorial we will use two data sets that are included in the phyloseq package. 2.1 Global Patterns The Global Patterns dataset (Caporaso et al. 2011) is a dataset available in the phyloseq package. These data contain samples from 25 different environmental samples and mock communities. The sampling depth of these samples averages 3.1 million total counts. We will use this dataset to work through the different normalization methods. The following lines load the relevant packages and data. library(tidyverse) library(phyloseq) data(&quot;GlobalPatterns&quot;) # examine phyloseq object GlobalPatterns ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 19216 taxa and 26 samples ] ## sample_data() Sample Data: [ 26 samples by 7 sample variables ] ## tax_table() Taxonomy Table: [ 19216 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 19216 tips and 19215 internal nodes ] 2.2 Colorectal cancer Another publicly available dataset is from a study on colorectal cancer (Kostic et al. 2012). See the abstract of this study below: The tumor microenvironment of colorectal carcinoma is a complex community of genomically altered cancer cells, nonneoplastic cells, and a diverse collection of microorganisms. Each of these components may contribute to carcinogenesis; however, the role of the microbiota is the least well understood. We have characterized the composition of the microbiota in colorectal carcinoma using whole genome sequences from nine tumor/normal pairs. Fusobacterium sequences were enriched in carcinomas, confirmed by quantitative PCR and 16S rDNA sequence analysis of 95 carcinoma/normal DNA pairs, while the Bacteroidetes and Firmicutes phyla were depleted in tumors. Fusobacteria were also visualized within colorectal tumors using FISH. These findings reveal alterations in the colorectal cancer microbiota; however, the precise role of Fusobacteria in colorectal carcinoma pathogenesis requires further investigation. This dataset is downloaded with the phyloseq package as a .biom file and can be loaded in using the following code, showing how to load qiime files as phyloseq objects. These data contain observations from 2505 taxa across 185 samples, which are categorized as ‘Tumor’ or ‘Healthy’. We remove samples that are not categorized into these two categories. # Load the data from a system file that is downloaded if the phyloseq package is installed filepath &lt;- system.file(&quot;extdata&quot;, &quot;study_1457_split_library_seqs_and_mapping.zip&quot;, package=&quot;phyloseq&quot;) kostic &lt;- microbio_me_qiime(filepath) ## Found biom-format file, now parsing it... ## Done parsing biom... ## Importing Sample Metdadata from mapping file... ## Merging the imported objects... ## Successfully merged, phyloseq-class created. ## Returning... # Some samples are labeled as &quot;none&quot; as a dignosis; remove these samples. # Save as an object with the un-normalized counts k_raw &lt;- subset_samples(kostic, DIAGNOSIS != &quot;None&quot;) k_raw ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 2505 taxa and 185 samples ] ## sample_data() Sample Data: [ 185 samples by 71 sample variables ] ## tax_table() Taxonomy Table: [ 2505 taxa by 7 taxonomic ranks ] 2.3 Pre-processing Quality Control and Filtering In addition to normalization, there are some steps we can perform that ideally remove technical artifacts from the sequencing process that only introduce noise. These filtering steps commonly consist of filtering out samples with a low total read depth and filtering out taxa that are rarely abundant. Let’s create a filtered version of the Global Patterns dataset. Note that there are only 26 samples, and all have a large library size, so we will not filter out any samples here. For taxa filtering, we will remove taxa that appear fewer than 5 times in more than half the samples. # Determine which taxa to remove filter_taxa &lt;- genefilter_sample(GlobalPatterns, filterfun_sample(function(x) x &gt; 5), A=0.5*nsamples(GlobalPatterns)) # Remove those taxa from the GlobalPatterns dataset # Save as an object with the un-normalized counts gp_raw &lt;- prune_taxa(filter_taxa, GlobalPatterns) gp_raw ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 219 taxa and 26 samples ] ## sample_data() Sample Data: [ 26 samples by 7 sample variables ] ## tax_table() Taxonomy Table: [ 219 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 219 tips and 218 internal nodes ] This decreases the number of taxa from 19216 to 219. This is not surprising, because this dataset contains samples from widely different locations (gut, soil, etc), and few taxa are shared among all samples and locations. One potential problem with this approach is the widely different locations, so it is possible that the remaining taxa could be some technical artifact, or could be a general ‘core’ set of taxa shared across the disparate environments. Additionally, let us save the total sampling depth as the variable depth in the metadata for the Global Patterns dataset. gp_raw@sam_data$depth &lt;- sample_sums(gp_raw) We can visualize technical artifacts of sapling depth is by looking at principal coordinates plots using the Bray-Curtis dissimilarity, coloring by sampling depth too see how much variation can be explained by the original sampling depth. gp_raw_dist &lt;- phyloseq::ordinate(gp_raw, &quot;PCoA&quot;, &quot;bray&quot;) plot_ordination(gp_raw, gp_raw_dist, color = &quot;depth&quot;, title = &quot;PCoA on Raw counts&quot;) We don’t see any extreme patterns with sampling depth, but this might additionally be due to the differences in different locations might have different sampling depths. This comparison might be more interesting when we only have one location we are sampling from. Repeat this same process for the kostic data. Use a prevalence filter to only include taxa that have nonzero counts in over 10% of samples. This results in 478 taxa across 185 samples. # Prevalence filter prevalenceThreshold &lt;- 0.10 * nsamples(k_raw) toKeep &lt;- apply(data.frame(otu_table(k_raw)), 1, function(taxa) return(sum(taxa &gt; 0) &gt; prevalenceThreshold)) k_raw &lt;- prune_taxa(toKeep, k_raw) k_raw ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 478 taxa and 185 samples ] ## sample_data() Sample Data: [ 185 samples by 71 sample variables ] ## tax_table() Taxonomy Table: [ 478 taxa by 7 taxonomic ranks ] # Save sampling depth as a sample variable k_raw@sam_data$depth &lt;- sample_sums(k_raw) k_raw_dist &lt;- phyloseq::ordinate(k_raw, &quot;PCoA&quot;, &quot;bray&quot;) plot_ordination(k_raw, k_raw_dist, color = &quot;depth&quot;, shape = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on Raw counts&quot;) When considering a normalization method, it is important to know if between sample (DA methods) or within sample (community) analysis is of interest. References "],["count-scaling-normalization-methods.html", "Chapter 3 Count Scaling Normalization methods 3.1 Total Sum scaling (TSS) 3.2 Cumulative sum scaling (CSS)", " Chapter 3 Count Scaling Normalization methods 3.1 Total Sum scaling (TSS) 3.1.1 About TSS The first method described is Total Sum Scaling (TSS). This method is also referred to as Total Count (TC), converting into proportions, or relative abundance. This is a scaling method to normalize the different library sizes across samples. For every entry in the count matrix, we scale by the total read depth of that sample. This converts the counts into the proportion of abundance present in each given sample. Though a more straightforward method, TSS normalization is not without its drawbacks. In microbiome data, it is common to have numerous low or zero-count observations, and that only a few most common OTUs contribute to the majority of the total sum of the sampling depth. These high-count, frequent, taxa could be an artifact of the sequencing step, where high abundance observations are preferentially sampled. Using these large counts can dominate the scaling factor for each sample. As seen below, we see that the scaling factor for each sample is completely dominated by ASV1, if that one taxon were not included in the sample, the scaling factor would be widely different. Sample ASV1 ASV2 ASV3 ASV4 TSS Scaling Factor Scaling factor w/o ASV1 Sample A 10314 34 8 12 10368 54 Sample B 824 23 13 20 880 56 Because this method does not account for the preferential sequencing over-abundance of ASV1 it is possible to see an increase in false positives. However, this is a widely used method, and one of the few normalization methods that completely accounts for differing library sizes, which can be an important consideration depending on the analysis goal. Community level-analysis, for example, can be library-size dependent (ordination, some dissimiliarity measures). 3.1.2 TSS R code 3.1.2.1 Function Here, we provide a wrapper function that will normalize a phyloseq object by Total Sum Scaling. We have the option of keeping the result as proportions (having values 0-1), or transforming to an equal sequencing depth so the results are counts per million. norm_TSS &lt;- function(ps, keep_prop = F){ # keep as proportions or convert to counts per million? scale &lt;- ifelse(keep_prop, 1, 1e6) # TSS function ps_normed &lt;- phyloseq::transform_sample_counts(ps, function(x) x * scale / sum(x)) return(ps_normed) } 3.1.2.2 TSS implementation on Global Patterns Using the above function, we apply this normalization to the Global Patterns data. gp_tss &lt;- norm_TSS(gp_raw) # rename the depth as the scaling factor sample_data(gp_tss)$scaling_factor &lt;- sample_data(gp_tss)$depth To see the differences between the un-normalized, raw data, and the TSS transformed normalized data, one possible way is to look at ordination plots. Microbiome data are high dimensional, so visualization directly of the data is difficult. Here, let us examine the principal coordinates plot using the Bray-Curtis dissimilarity. First calculate the distance matrices, using the phyloseq function ordinate() gp_raw_dist &lt;- phyloseq::ordinate(gp_raw, &quot;PCoA&quot;, &quot;bray&quot;) gp_tss_dist &lt;- phyloseq::ordinate(gp_tss, &quot;PCoA&quot;, &quot;bray&quot;) Now plot the two ordinations. Even before normalization, the different communities are clearly clustered. (Note to self: perhaps choose a different dataset to use for walk-through, currently using Global Patterns since it is small and quick for computations, but harder to see differences.) plot_ordination(gp_raw, gp_raw_dist, color = &quot;SampleType&quot;, title = &quot;PCoA on Raw Counts&quot;) + plot_ordination(gp_tss, gp_tss_dist, color = &quot;SampleType&quot;, title = &quot;PCoA on TSS normalized counts&quot;) + plot_layout(guides = &#39;collect&#39;) We can also compare the values of the distance matrices before and after normalization to see how the normalization method is impacting different types of points. # Function to visualize potential differences and changes after normalization methods plot_norm_changes &lt;- function(data_norm, data_raw, dist_method = &quot;bray&quot;, x_lab = &quot;raw&quot;, y_lab = &quot;norm&quot;, title = &quot;Plot title&quot;){ # calculate the Bray-Cutris distance matrix for the raw data, the normalized data, # and calculate the pairwise difference between the original library sizes between samples plot &lt;- data.frame(raw = as.numeric(phyloseq::distance(data_raw, dist_method)), norm = as.numeric(phyloseq::distance(data_norm, dist_method)), diff = as.numeric(dist(get_variable(data_raw, &quot;depth&quot;)))) %&gt;% ggplot(aes(x = raw, y = norm, color = diff)) + geom_point() + geom_abline() + ggtitle(title) + xlab(x_lab) + ylab(y_lab) + labs(color = &quot;Pairwise difference in sampling depth&quot;)+ xlim(c(0,1)) + ylim(c(0,1)) return(plot) } plot_norm_changes(gp_tss, gp_raw, x_lab = &quot;Raw counts&quot;, y_lab = &quot;TSS props&quot;, title = &quot;Distance metric comparision between Raw counts and TSS normalized counts &quot;) Points below the line are pairs of samples that are marked as more similar after normalization. Points above the line are marked as more different after normalization. Values closer to 1 are ‘more different’. Unsurprisingly, pairs that had larger original differences in sampling depth were marked as more different on the raw, un-normalized data, and became marked as more similar after TSS normalization. 3.1.2.3 TSS on Kostic data k_tss &lt;- norm_TSS(k_raw) k_raw_dist &lt;- phyloseq::ordinate(k_raw, &quot;PCoA&quot;, &quot;bray&quot;) k_tss_dist &lt;- phyloseq::ordinate(k_tss, &quot;PCoA&quot;, &quot;bray&quot;) # Ordination with Diagnosis as color plot_ordination(k_raw, k_raw_dist, color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on Raw Counts&quot;) + plot_ordination(k_tss, k_tss_dist, color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on TSS normalized counts&quot;) + plot_layout(guides = &#39;collect&#39;) # Ordination with original sampling depth as color plot_ordination(k_raw, k_raw_dist, color = &quot;depth&quot;, title = &quot;PCoA on Raw Counts&quot;) + plot_ordination(k_tss, k_tss_dist, color = &quot;depth&quot;, title = &quot;PCoA on TSS normalized counts&quot;) + plot_layout(guides = &#39;collect&#39;) plot_norm_changes(k_tss, k_raw, x_lab = &quot;Raw counts&quot;, y_lab = &quot;TSS props&quot;, title = &quot;Distance metric comparision between Raw counts and TSS normalized counts &quot;) 3.2 Cumulative sum scaling (CSS) 3.2.1 About CSS Cumulative sum scaling is a scaling normalization method, developed for marker gene sequencing, It is intended to account for undersampling and correct biases from preferentially amplified features in a sample-specific manner (Paulson et al. 2013). This method assumes that count distributions should be roughly equivalent and independent to each other up to the given quantile which is chosen as the smallest value at which instability is found. This method is an extension to UQ scaling where a quantile is specified. If there is high count variability the assumption may not be met. This is not a method that accounts for compositionality. CSS normalization initially showed improvements in separating samples biologically in ordination, it was shown to be an artifact of unequal application of log transformation across methods (Costea et al. 2014). 3.2.2 CSS R code 3.2.2.1 Function norm_CSS &lt;- function(ps){ require(metagenomeSeq) # Convert to metagenomeSeq data type ps.metaG&lt;-phyloseq_to_metagenomeSeq(ps) p_stat = cumNormStatFast(ps.metaG) ps.metaG = cumNorm(ps.metaG, p = p_stat) ps.metaG.norm &lt;- MRcounts(ps.metaG, norm = T) # Convert back to phyloseq with normalized counts otu &lt;- otu_table(ps.metaG.norm, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- normFactors(ps.metaG)/1e6 tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_CSS &lt;- phyloseq(otu,sam,tax,phy) return(ps_CSS) } 3.2.2.2 CSS on Global Patterns Perform CSS normalization: gp_css &lt;- norm_CSS(gp_raw) ## Default value being used. View how TMM normalization changes distance metrics differently than raw counts. plot_norm_changes(gp_css, gp_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;CSS&quot;, title = &quot;Distance metric comparision between CSS normalization and Raw counts&quot;) plot_norm_changes(gp_css, gp_tss, x_lab = &quot;TSS&quot;, y_lab = &quot;CSS&quot;, title = &quot;Distance metric comparision between CSS normalization and TSS normalization&quot;) CSS normalization appears to consider pairs as more different than TSS normalization, and pairs with high sequencing depth differences even more so. 3.2.2.3 CSS on Kostic data k_css &lt;- norm_CSS(k_raw) ## Default value being used. plot_ordination( k_css, phyloseq::ordinate(k_css, &quot;PCoA&quot;, &quot;bray&quot;) , color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on Raw data&quot; ) / plot_ordination( k_css, phyloseq::ordinate(k_css, &quot;PCoA&quot;, &quot;bray&quot;) , color = &quot;depth&quot;, title = &quot;PCoA on Raw data&quot; ) plot_norm_changes(k_css, k_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;CSS&quot;, title = &quot;Distance metric comparision between CSS normalization and Raw counts&quot;) References "],["subsampling-methods.html", "Chapter 4 Subsampling Methods 4.1 Rarefying", " Chapter 4 Subsampling Methods 4.1 Rarefying 4.1.1 About Rarefying Rarefying is another common normalization technique that standardizes the library size across samples that was originally used in ecology. This method standardizes the read depth across all samples. To perform this method we first choose a minimum library size. Looking at rarefaction/collectors curves, or using a certain percentile can guide choosing this cutoff. Then all samples that have a read depth below this cutoff are discarded. Thus this method has a built-in filtering step. Next, we sample without replacement of the size of the chosen cutoff. It can be a standalone method or combined with other methods and transformations. This is a very commonly used method, but it has also been criticized (McMurdie and Holmes 2014). First of all, it throws away valid data, and this results in a loss of power and an increase in false positives. Rare taxa can be removed in this approach too. It is however encouraged when we have widely different library sizes as it can lower the false discovery rate (Weiss et al. 2017), and has also been shown to perform well in community-level analysis (McKnight et al. 2019), as it completely standardizes the read count depth, and some methods are sensitive to differences in read count. Rarefying has been shown to separate by biological signal in ordination methods based on presence/absence. 4.1.2 Rarefying walkthrough Consider the following example dataset. Table 4.1: Example dataset - Raw Counts Taxa Sample 1 Sample2 Sample 3 Taxon 1 132 103 11 Taxon 2 7 48 3 Taxon 3 0 2 1 Taxon 4 23 15 2 Taxon 5 71 80 5 To normalize this data by rarefying, we first choose a minimum sampling depth, or in other words, the minimum column sum. After normalizing, the column sum of all samples will be this size. Any samples that are below this minimum will be dropped from analysis. We choose 200 as our minimum here. In practice the counts will normally be much higher. In this example, our sampling depths are 223, 248, and 20. Thus the third sample will be dropped in this procedure. Since we are randomly sampling, we need to keep track of the seed so this process is reproducible. set.seed(525) We then randomly sample taxa from each sample/column in the proportions corresponding to the raw counts. The below table shows the table from above after we rarefy. Notice that the rare Taxon 3, which originally was present in Sample 1 and 3 is now completely removed from the analysis. Table 4.2: Rarefied counts Taxa Sample 1 Sample2 Taxon 1 115 90 Taxon 2 6 39 Taxon 3 0 0 Taxon 4 18 11 Taxon 5 61 60 4.1.3 Rarefying R code We now show the process of rarefying a real dataset. The following function returns a rarefied phyloseq object. We can either pass in the minimum sampling depth as a second argument, or use the default minimum depth of the samples. norm_rarefy &lt;- function(phyloseq, depth = min(sample_sums(phyloseq))){ return(phyloseq::rarefy_even_depth(phyloseq,sample.size = depth)) } 4.1.3.1 Rarefying on Global Patterns We use the above function to rarefy the Global Patterns data. The first difficulty is choosing a minimum sampling depth. The Global Patterns dataset already has a very high sampling depth for all samples, so we will chose the lowest as the minimum depth to rarefy to.Since we chose the minimum sampling depth, no samples have been dropped. In data sets where we have low sampling depth there is a balance between how many samples to drop and how low to set the minimum depth to. gp_rare &lt;- norm_rarefy(gp_raw) ## You set `rngseed` to FALSE. Make sure you&#39;ve set &amp; recorded ## the random seed of your session for reproducibility. ## See `?set.seed` ## ... We can check that indeed all samples now have the same sampling depth, which is 15905. Note that the highest sampling depth in this dataset was almost 2 million, so we have discarded a lot of data to reduce to 15905. max(sample_sums(gp_raw)) ## [1] 1842380 sample_sums(gp_rare) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr F21Plmr ## 15905 15905 15905 15905 15905 15905 15905 15905 ## M31Tong M11Tong LMEpi24M SLEpi20M AQC1cm AQC4cm AQC7cm NP2 ## 15905 15905 15905 15905 15905 15905 15905 15905 ## NP3 NP5 TRRsed1 TRRsed2 TRRsed3 TS28 TS29 Even1 ## 15905 15905 15905 15905 15905 15905 15905 15905 ## Even2 Even3 ## 15905 15905 We can again compare the PCoA plots between rarefied and raw counts, coloring by sample type to view clusters. plot_ordination(gp_raw, phyloseq::ordinate(gp_raw, &quot;PCoA&quot;, &quot;bray&quot;), color = &quot;SampleType&quot;, title = &quot;PCoA on Raw counts&quot;) + plot_ordination(gp_rare, phyloseq::ordinate(gp_rare, &quot;PCoA&quot;, &quot;bray&quot;), color = &quot;SampleType&quot;, title = &quot;PCoA on Rarefied counts&quot;) + plot_layout(guides = &#39;collect&#39;) Now examine how the distance matrices change before/after normalization. We see a similar pattern to TSS when distance matrices calculated from rarefied counts are compared to those from the raw counts. # Identify any samples filtered in rarefying process rare_samples &lt;- sample_names(gp_rare) gp_raw_match &lt;- prune_samples(rare_samples, gp_raw) plot_norm_changes(gp_rare, gp_raw_match, x_lab = &quot;Raw counts&quot;, y_lab = &quot;Rarefied counts&quot;, title = &quot;Distance metric comparision between Raw counts and Rarefied counts &quot;) ## Compare to tss gp_tss_match &lt;- norm_TSS(gp_raw_match) plot_norm_changes(gp_rare, gp_tss, x_lab = &quot;TSS&quot;, y_lab = &quot;Rarefied&quot;, title = &quot;Distance metric comparision between TSS normalization and Rarefied counts &quot;) 4.1.4 Rarefying on Kostic Data k_rare &lt;- norm_rarefy(k_raw) ## You set `rngseed` to FALSE. Make sure you&#39;ve set &amp; recorded ## the random seed of your session for reproducibility. ## See `?set.seed` ## ... ## 128OTUs were removed because they are no longer ## present in any sample after random subsampling ## ... plot_norm_changes(k_rare, k_raw, x_lab = &quot;Raw counts&quot;, y_lab = &quot;Rarefied counts&quot;, title = &quot;Distance metric comparision between Raw counts and Rarefied counts &quot;) plot_norm_changes(k_rare, k_tss, x_lab = &quot;TSS&quot;, y_lab = &quot;Rarefied&quot;, title = &quot;Distance metric comparision between TSS normalization and Rarefied counts &quot;) References "],["library-size-and-composition-scaling-methods.html", "Chapter 5 Library Size and Composition Scaling Methods 5.1 DESeq 5.2 GMPR 5.3 TMM (edgeR)", " Chapter 5 Library Size and Composition Scaling Methods 5.1 DESeq 5.1.1 About DESeq The DESeq2 package includes a normalization method for adjusting for differing library sizes across samples (Anders and Huber 2010). This method also can account for differences in library composition. A scaling factor to normalize each sample takes into account both library size and library composition. This method has also been called MED, RLE, or DESeq in the literature. DESeq2 first takes the natural logarithm of every entry in the count matrix. Due to this, all entries with zero will be set to negative infinity. Next, the row average is calculated (geometric average), so we have a vector of average counts for each taxon. Taking the log first should avoid undue influence by extreme outliers. All taxa with an average of infinity are removed. This step will remove all taxa with zero read count in one or more samples. This can be a problem in microbiome data. Next, we subtract the average log value from the log(counts), this gives a log ratio. This is equivalent to the ratio of the reads in each sample to the average across all samples. Next, we calculate the median of the log-ratios for each sample. These medians are converted to scaling factors for each sample by exponentiation. An extension of this method, denoted ‘poscounts’, has been suggested, which instead of taking the geometric mean of the logged counts for each taxon, we take the n-th root of the product of the non-zero counts. This method assumes that the taxon of median absolute abundance is not differentially abundant, which is more likely true for the RNA-Seq it was developed for, but may not be true for microbiome studies, especially when there are more study groups, or we are analyzing higher taxonomic levels. An additional option can be used to perform a variance stabilizing transformation on the count matrix before normalizing with the above size factors. This method calculates a dispersion-mean relationship and then transforms the data. The result ideally is an abundance matrix that is approximately homoskedastic or constant variance across the range of mean values. The package also includes an option for a ‘rlog’ transform, which they recommend over the variance stabilizing method in the case when there is a large difference in library sizes. If differential abundance is of interest to calculate, DESeq uses a negative binomial distribution to model differential abundances. It is possible to provide the size factors calculated by another method to DESeq to perform differential analysis. 5.1.1.1 DESeq walkthrough Consider again this example dataset. Table 5.1: Example dataset - Raw Counts Taxa Sample 1 Sample2 Sample 3 Taxon 1 132 103 11 Taxon 2 7 48 3 Taxon 3 0 2 1 Taxon 4 23 15 2 Taxon 5 71 80 5 The first step in DESeq normalization is to take the natural log of each entry in the count matrix. Table 5.2: Step 1: Logged Raw Counts Taxa Sample 1 Sample2 Sample 3 Taxon 1 4.882802 4.6347290 2.3978953 Taxon 2 1.945910 3.8712010 1.0986123 Taxon 3 -Inf 0.6931472 0.0000000 Taxon 4 3.135494 2.7080502 0.6931472 Taxon 5 4.262680 4.3820266 1.6094379 Notice the one entry with zero counts is marked as negative infinity. Next, the average of the logged values is taken across the samples. This results in an average log value for each taxa. This averaged log value is helpful for normalizing because it is not overwhelmed by outliers. Table 5.3: Step 2: Average Logged Counts Taxa Average Log Counts Taxon 1 3.971809 Taxon 2 2.305241 Taxon 3 -Inf Taxon 4 2.178897 Taxon 5 3.418048 The row for the taxa that contained a zero on one of the samples is negative infinity. This taxon is now excluded from the following steps. This step results in removing any taxa that have zero counts from being considered to contribute to calculating the scaling factors. The next step is to subtract the average log values (step 2) from the log of the raw counts (step 1), only including rows that were not filtered. This step shows which samples have counts in a sample higher or lower than the average. Table 5.4: Step 3: Log Ratio of reads in each sample to average across all samples Taxa Sample 1 Sample 2 Sample 3 Taxon 1 4.88 - 3.97 = 0.91 4.63 - 3.97 = 0.66 2.40 - 3.97 = -1.57 Taxon 2 1.95 - 2.31 = -0.36 3.97 - 2.31 = 1.66 1.10 - 2.31 = -1.31 Taxon 4 3.14 - 2.18 = 0.96 2.71 - 2.18 = 0.53 0.69 - 2.18 = -1.49 Taxon 5 4.26 - 3.42 = 0.84 4.38 - 3.42 = 0.96 1.61- 3.42 = -1.81 Next, to calculate the scaling factors for each sample, we take the median of the log ratios calculated in the above step. Like using logs, calculating medians avoids the influence of outlier taxa that put undue influence on the scaling factor. Table 5.5: Step 4: Calculating medians per sample Taxa Sample 1 Sample 2 Sample 3 Taxon 1 0.910000 0.660000 -1.5700000 Taxon 2 -0.360000 1.660000 -1.3100000 Taxon 4 0.960000 0.530000 -1.4900000 Taxon 5 0.840000 0.960000 -1.8100000 Median 0.875000 0.810000 -1.5300000 Exponentiated Median 2.398875 2.247908 0.2165357 Finally, we normalize the data, by exponentiating the median log ratios for each sample, which are the final scaling factors. We then divide all the raw counts in a sample by the sample’s scaling factor. Table 5.6: Step 5: DESeq Normalized Counts Taxa Sample 1 Sample2 Sample 3 Taxon 1 55.025787 45.8203808 50.799945 Taxon 2 2.918034 21.3531872 13.854530 Taxon 3 0.000000 0.8897161 4.618177 Taxon 4 9.587827 6.6728710 9.236354 Taxon 5 29.597203 35.5886453 23.090884 Since Taxon 3 had a zero count in sample 1, it was excluded from the calculation of scale factors. The above example dataset may not be characteristic of microbiome datasets. Microbiome datasets are zero-inflated, meaning that there are numerous zero counts in the raw count matrix. Even up to 80-90% of the counts in a microbiome datset can be zero. Because of this, if the zero-inflation in the datseta is not accounted for, very few, or even perhaps none of the taxa will contribute to calculating the scaling factor. One option so that all of the taxa are included in the calculation is to add a pseudocount so none of the counts are zero. Another option is the poscounts option, which is encouraged for microbiome data. Instead of taking the average of the logged counts, it takes the \\(n\\)th root of the non-zero counts. This replaces step 2 in this example. 5.1.2 DESeq R Code 5.1.2.1 Function Here we provide two normalization functions implemented in R using DESeq methods. The first calculates the RLE normalization using the poscounts option for microbiome data. The second calculates the variance stabilizing transformation. norm_DESeq_RLE_poscounts &lt;- function(ps, group = 1){ require(DESeq2, quietly = T) # keep arbitrary design for normalization # Convert to DESeq object ps_dds &lt;- phyloseq_to_deseq2(ps, ~1) # Calculate the size factors (scaling) ps_dds &lt;- estimateSizeFactors(ps_dds, type = &quot;poscounts&quot;) # Extract counts counts &lt;- DESeq2::counts(ps_dds, normalized = T) # Convert back to phyloseq otu &lt;- otu_table(counts, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- sizeFactors(ps_dds) tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_DESeq &lt;- phyloseq(otu,sam,tax,phy) return(ps_DESeq) } norm_DESeq_vs &lt;- function(ps, group = 1){ require(DESeq2, quietly = T) ps_dds &lt;- phyloseq_to_deseq2(ps, ~ 1) ps_dds &lt;- estimateSizeFactors(ps_dds, type = &quot;poscounts&quot;) # Variance transformation ps_dds &lt;- estimateDispersions(ps_dds) abund &lt;- getVarianceStabilizedData(ps_dds) # don’t allow deseq to return negative counts # add the minimum count to all values # another option is to replace negative counts with 0 abund &lt;- abund + abs(min(abund)) otu &lt;- otu_table(abund, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_DESeq &lt;- phyloseq(otu,sam,tax,phy) return(ps_DESeq) } 5.1.2.2 DESeq implemented on Global Patterns Perform DESeq RLE normalization as well as DESeq variance stabilized transformation on Global Patterns: gp_deseq_rle &lt;- norm_DESeq_RLE_poscounts(gp_raw) gp_deseq_vs &lt;- norm_DESeq_vs(gp_raw) Examine principal coordinate plots between raw data and both DESeq normalized data. # First calculate distance matrices gp_rle_dist &lt;- phyloseq::ordinate(gp_deseq_rle, &quot;PCoA&quot;, &quot;bray&quot;) gp_vs_dist &lt;- phyloseq::ordinate(gp_deseq_vs, &quot;PCoA&quot;, &quot;bray&quot;) # Plot ordinations plot_ordination(gp_raw, gp_raw_dist, color = &quot;SampleType&quot;, title = &quot;PCoA on Raw data&quot;) + plot_ordination(gp_deseq_rle, gp_rle_dist, color = &quot;SampleType&quot;, title = &quot;PCoA on RLE&quot;) + plot_ordination(gp_deseq_vs, gp_vs_dist, color = &quot;SampleType&quot;, title = &quot;PCoA on VST&quot;) + plot_layout(guides = &#39;collect&#39;) See how dissimilarity matrices differ from raw counts and each DESeq transformation. plot_norm_changes(gp_deseq_rle, gp_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;RLE&quot;, title = &quot;Distance metric comparision between RLE normalization and Raw counts &quot;) / plot_norm_changes(gp_deseq_vs, gp_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;VST&quot;, title = &quot;Distance metric comparision between VST normalization and Raw counts &quot;) + plot_layout(guides = &#39;collect&#39;) 5.1.2.3 DESeq implemented on Kostic dataset k_deseq_rle &lt;- norm_DESeq_RLE_poscounts(k_raw) k_deseq_vs &lt;- norm_DESeq_vs(k_raw) # First calculate distance matrices k_rle_dist &lt;- phyloseq::ordinate(k_deseq_rle, &quot;PCoA&quot;, &quot;bray&quot;) k_vs_dist &lt;- phyloseq::ordinate(k_deseq_vs, &quot;PCoA&quot;, &quot;bray&quot;) # Plot ordinations plot_ordination(k_raw, k_raw_dist, color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on Raw data&quot;) + plot_ordination(k_deseq_rle, k_rle_dist, color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on RLE&quot;) + plot_ordination(k_deseq_vs, k_vs_dist, color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on VST&quot;) + plot_layout(guides = &#39;collect&#39;) plot_norm_changes(k_deseq_rle, k_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;RLE&quot;, title = &quot;Distance metric comparision between RLE normalization and Raw counts &quot;) / plot_norm_changes(k_deseq_vs, k_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;VST&quot;, title = &quot;Distance metric comparision between VST normalization and Raw counts &quot;) + plot_layout(guides = &#39;collect&#39;) 5.2 GMPR 5.2.1 About GMPR A recent extension of the RLE DESeq method is the Geometric mean of Pairwise ratios (GMPR) approach (Chen et al. 2018). This method reverses the steps of RLE, and instead calculates the median count ratio of the non-zero counts between pairs of samples as although only a small number of taxa are likely to be shared for every sample, it is more likely that there are many shared taxa between pairs. It then uses the pairwise results to calculate the size factor for each sample. This method has slow computation, but is robust to differential and outlier taxa. The size factors can be inputted to DESeq and a VST transformation applied additionally. It is a newer method, and has unfortunately not been included in many benchmarking studies, although initial results show it to be more powerful than DESeq, not surprisingly, as it uses more data, as zero counts do not need to be discarded. It assumes there is a large invariant portion of the count data, similar to other methods. 5.2.2 GMPR Walkthough Consider the following zero-inflated dataset. Notice that there are no taxa that are present in every sample. Table 5.7: Example dataset - Raw Counts Taxa Sample 1 Sample2 Sample 3 Taxon 1 132 103 0 Taxon 2 0 48 74 Taxon 3 0 2 0 Taxon 4 23 0 35 Taxon 5 71 80 0 Taxon 6 0 96 82 The first step in GMPR normalization is to calculate the pairwise median count ratio between samples. We first calculate the scaling factor for sample 1. The pairwise comparisons we need to make are between sample 1 and sample 2 as well as sample 1 and sample 3. For both pairs, we calculate the ratio of the counts of taxa that the pair shares. Between sample 1 and sample 2, the shared taxa are 1 and 5. Between sample 1 and sample 3, taxon 4 is the only one shared. Table 5.8: Example dataset - Raw Counts Taxa Sample 1 Sample2 Sample 3 Count ratio between 1 and 2 Count ratio between 1 and 3 Taxon 1 132 103 0 132/103 Taxon 2 0 48 74 Taxon 3 0 2 0 Taxon 4 23 0 35 23/35 Taxon 5 71 80 0 71/80 Taxon 6 0 96 82 Then we calculate the median of the ratios between each pair. Between sample 1 and 2, the two ratios between shared taxa are (132/103, 71/80). Then the median of those is 1.085. Between sample 1 and 3, there is only one shared taxa, so the median is 0.657. Finally to find the scaling factor for sample 1, we calculate the geometric mean of the two medians of the pairwise shared taxa ratios. In this case it is the geometric mean of 1.085 and 0.657, which equals 0.844. This is the scaling factor for sample 1. We now repeat this process to find the scaling factor for sample 1 and sample 3. 5.2.3 GMPR R Code 5.2.3.1 Function The following provides the R code to implement GMPR normalization in R. We can additionally specify the number of taxa that should be shared between paired samples as well as the minimum count labeled as nonzero. The default values are 4 and 2 respectively. norm_GMPR &lt;- function(ps, intersect_no = 4, min_ct = 2){ require(GMPR, quietly = T) # Convert data to correct format for GMPR function otu &lt;- as(otu_table(ps), &quot;matrix&quot;) if(taxa_are_rows(ps)){otu &lt;- t(otu)} otu_df = as.data.frame(otu) otu.tab &lt;- matrix(otu, ncol = ncol(otu)) # calculate scaling factor # OTU matrix must be a data frame where OTUs are arranged in columns and samples as rows gmpr.size.factor &lt;- GMPR::GMPR(OTUmatrix = otu_df, intersect_no = intersect_no, min_ct = min_ct) # normalize otu.tab.norm &lt;- t(otu / (gmpr.size.factor)) # convert back to PS otu_ps &lt;- otu_table(otu.tab.norm, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- gmpr.size.factor tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_GMPR &lt;- phyloseq(otu_ps,sam,tax,phy) return(ps_GMPR) } 5.2.3.2 GMPR implementation on Global Patterns # Normalize gp_gmpr &lt;- norm_GMPR(gp_raw) # Compare to other methods like before plot_norm_changes(gp_gmpr, gp_raw, x_lab = &quot;GMPR&quot;, y_lab = &quot;Raw Counts&quot;, title = &quot;GMPR normalization vs Raw Counts&quot;) plot_norm_changes(gp_gmpr, gp_tss, x_lab = &quot;TSS&quot;, y_lab = &quot;GMPR&quot;, title = &quot;TSS vs GMPR&quot;) plot_norm_changes(gp_gmpr, gp_deseq_rle, x_lab = &quot;DESeq&quot;, y_lab = &quot;GMPR&quot;, title = &quot;DESeq vs GMPR&quot; ) 5.3 TMM (edgeR) 5.3.1 About TMM TMM (Trimmed median of m-values) is another method borrowed from RNA-Seq analysis, and implemented in edgeR (Robinson and Oshlack 2010). This method uses, or calculates a reference sample, and compares all other samples to the reference sample. The size factor is the mean of the log-ratios after excluding the highest count taxa and taxa with the largest fold change. As taxa with zero counts are excluded, a pseudo count is needed. Additionally, there is the TMMwsp option which is encouraged as it is more robust to zero counts. Positive counts are reused to increase the number of features when we compared. The singleton positive counts are paired up in decreasing order of size and then a modified TMM method is applied to the re-ordered libraries. 5.3.2 EdgeR TMM T code 5.3.2.1 Function norm_TMM &lt;- function(physeq, group = 1, method=&quot;TMM&quot;, pseudocount = 1, ...){ require(&quot;edgeR&quot;, quietly = T) require(&quot;phyloseq&quot;, quietly = T) # Enforce orientation. if( !taxa_are_rows(physeq) ){ physeq &lt;- t(physeq) } x = as(otu_table(physeq), &quot;matrix&quot;) # Add one to protect against overflow, log(0) issues. x = x + pseudocount # Check `group` argument if( identical(all.equal(length(group), 1), TRUE) &amp; nsamples(physeq) &gt; 1 ){ # Assume that group was a sample variable name (must be categorical) group = get_variable(physeq, group) } # Define gene annotations (`genes`) as tax_table taxonomy = tax_table(physeq, errorIfNULL=FALSE) if( !is.null(taxonomy) ){ taxonomy = data.frame(as(taxonomy, &quot;matrix&quot;)) } # Now turn into a DGEList y = DGEList(counts=x, group=group, genes=taxonomy, remove.zeros = TRUE) # Calculate the normalization factors d = edgeR::calcNormFactors(y, method=method) # Check for division by zero inside `calcNormFactors` if( !all(is.finite(d$samples$norm.factors)) ){ stop(&quot;Something wrong with edgeR::calcNormFactors on this data, non-finite $norm.factors, consider changing `method` argument&quot;) } scalingFactor &lt;- d$samples$norm.factors * d$samples$lib.size / 1e6 dataNormalized &lt;- t(t(otu_table(physeq)) / scalingFactor) #dataNormalized &lt;- cpm(d) otu &lt;- otu_table(dataNormalized, taxa_are_rows = T) sam &lt;- access(physeq, &quot;sam_data&quot;) sam$scaling_factor &lt;- scalingFactor tax &lt;- access(physeq, &quot;tax_table&quot;) phy &lt;- access(physeq, &quot;phy_tree&quot;) ps_edgeR &lt;- phyloseq(otu,sam,tax,phy) return(ps_edgeR) } 5.3.2.2 TMM implementation on Global Patterns Perform normalization: gp_tmm &lt;- norm_TMM(gp_raw) View PCoA plots plot_ordination(gp_tmm, phyloseq::ordinate(gp_tmm, &quot;PCoA&quot;, &quot;bray&quot;) , color = &quot;SampleType&quot;, title = &quot;PCoA on Raw data&quot;) View how TMM normalization changes distance metrics differently than raw counts. plot_norm_changes(gp_tmm, gp_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;TMM&quot;, title = &quot;Distance metric comparision between TMM normalization and Raw counts &quot;) 5.3.2.3 TMM implementation on Kostic data k_tmm &lt;- norm_TMM(k_raw) plot_ordination( k_tmm, phyloseq::ordinate(k_tmm, &quot;PCoA&quot;, &quot;bray&quot;) , color = &quot;DIAGNOSIS&quot;, title = &quot;PCoA on Raw data&quot; ) / plot_ordination( k_tmm, phyloseq::ordinate(k_tmm, &quot;PCoA&quot;, &quot;bray&quot;) , color = &quot;depth&quot;, title = &quot;PCoA on Raw data&quot; ) plot_norm_changes(k_tmm, k_raw, x_lab = &quot;Raw&quot;, y_lab = &quot;TMM&quot;, title = &quot;Distance metric comparision between TMM normalization and Raw counts &quot;) References "],["wrench.html", "Chapter 6 Wrench", " Chapter 6 Wrench Wrench is a recent normalization method developed for microbiome data (Kumar et al. 2018) that uses an empirical Bayes Normalization approach. This method includes compositional bias correction for sparse datasets. This method uses a hurdle log-normal distribution to estimate the normalization factors (the location estimate for the group). For this method, we assume abundances are drawn from a hurdle Log-Gaussian distribution, and the scaling factor used is essentially the location estimate for the group. norm_wrench &lt;- function(ps, group_col){ require(Wrench, quietly = T) if( identical(all.equal(length(group_col), 1), TRUE) &amp; nsamples(ps) &gt; 1 ){ # Assume that group was a sample variable name (must be categorical) group = get_variable(ps, group_col) } otu_tab &lt;- otu_table(ps) W &lt;- wrench(otu_tab, group) compositionalFactors &lt;- W$ccf normalizationFactors &lt;- W$nf normed_otu &lt;- otu_tab/(normalizationFactors/1e6) otu &lt;- otu_table(normed_otu, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- normalizationFactors tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_wrench &lt;- phyloseq(otu,sam,tax,phy) return(ps_wrench) } References "],["log-ratio-transformation-methods.html", "Chapter 7 Log Ratio Transformation Methods 7.1 ANCOM II 7.2 ANCOM BC 7.3 ALDEx2", " Chapter 7 Log Ratio Transformation Methods The log-ratio based methodology developed by Aitchison in the 1980s is useful for analyzing compositional data (Aitchison 1982). Taking the logarithm of ratios can be an appropriate transformation for compositional data, so standard statistical tests can be appropriate again. This transformation removes the issue of standardizing/normalizing different sampling depths. The sampling depth for a given sample will not distort the biological covariance or correlation structure. This log-ratio method has a drawback, which is the decision of how to define the denominator. One approach to this problem is to use one sample as the reference. This sample should be ‘representative’. The log-ratio transformation is then the ratio of every other taxon to that representative sample. Of course, knowledge of what makes a sample representative is hard to come by and often unknown, and subsequent results can be affected by this choice. This method is frequently called the additive log-ratio approach (alr). The alternative approach is to use the data to create a pseudo-reference sample. This pseudo-reference sample is the geometric mean of the counts of all taxa. This is called the centered log-ratio method. While promising, these log-ratio methods have drawbacks in practice. Microbiome data are often incredibly sparse, with up to 80-90% of count matrices containing zero counts. For ratio transformations, if we have sparse data, the geometric mean can be zero. Then the ratio is undefined, and further, so is the logarithm of zero count taxa. One solution to this is adding a small pseudo count to every element in the data. This removes problems occurring from having zero counts in the data, but there is not a clear best choice of what pseudo count to use, and it the choice can impact downstream results. 7.1 ANCOM II ANCOM-II (Kaul et al. 2017) is a log-ratio approach for accounting for zero-inflation in log-ratio transformations. It uses a log-ratio transformation instead of library size normalization. If there are \\(m\\) total taxa, this method performs \\(m-1\\) differential abundance tests, each one using a different taxa as the reference taxa. Before the alr log-ratio transformation is performed, this method identifies and adjusts for three types of zeros that can exist. These three types of zeros are outlier zeros, structural zeros, and sampling zeros. Outlier zeros are zeros caused from some extraneous reasons separate from below detection limits due to sampling depth. To account for these zeros, they are replaced by NA in the data. Secondly, structural zeros are zeros which are zero due to the nature of the groups. If a taxa is structurally zero in a group, it is marked as automatically differentially abundant, and removed from further analysis. Finally, sampling zeros are other zeros that are zero perhaps caused by sampling depth. These are imputed by a small pseudocount. This method thus takes into account zero-inflation and compositionality. After this correction of zeros, the data are log ratio transformed using each possible taxa as the reference sample. Tests for differential abundance are performed on each of iterations of the log-ratio transformed data, and the count of times the resulting test is significant is used to determining overall significance. It is possible that no outlier or structural zeros are detected, so as a normalization/transformation technique, this method can be equivalent to alr or clr transoformation on raw data with a pseudocount. ## Load the Ancom function (it&#39;s not a package) devtools::source_url(&quot;https://raw.githubusercontent.com/FrederickHuangLin/ANCOM/master/scripts/ancom_v2.1.R&quot;) norm_ANCOM2 &lt;- function(ps, group_var, outlier.replace = TRUE) { feature_table &lt;- as.data.frame(otu_table(ps)) groups &lt;- as.factor(get_variable(ps, group_var)) meta_data &lt;- data.frame(sample_id = paste0(&quot;sample&quot;, seq(dim(raw)[2])), groups = groups) #rownames(meta_data) &lt;- meta_data$sample_id colnames(feature_table) &lt;- meta_data$sample_id # The main ANCOM2 zero-classification function prepro &lt;- feature_table_pre_process( feature_table, meta_data, sample_var = &quot;sample_id&quot;, group_var = &quot;groups&quot;, out_cut = 0.05, zero_cut = 0.9, lib_cut = 0, neg_lb = FALSE ) feature_table &lt;- prepro$feature_table meta_data &lt;- prepro$meta_data struc_zero &lt;- prepro$structure_zeros # OTU table transformation: # (1) Discard taxa with structural zeros (if any); (2) Add pseudocount (1) and take logarithm. if (!is.null(struc_zero)) { num_struc_zero = apply(struc_zero, 1, sum) comp_table = feature_table[num_struc_zero == 0,] } else{ comp_table = feature_table } comp_table &lt;- log(as.matrix(comp_table) + 1) # Replace outlier zeros with the mean of the rest genes in each if (outlier.replace) { for (col in 1:ncol(comp_table)) { comp_table[is.na(comp_table[, col]), col] &lt;- mean(comp_table[, col], na.rm = T) } } # Calculate ratio n_taxa &lt;- dim(comp_table)[1] taxa_id &lt;- rownames(comp_table) n_samp &lt;- dim(comp_table)[2] log_geo_mean &lt;- apply(comp_table, 2, function(x) { mean(x, na.rm = T) }) mu_group_1 &lt;- mean(log_geo_mean[groups == levels(groups)[1]]) mu_group_2 &lt;- mean(log_geo_mean[groups == levels(groups)[2]]) mu &lt;- rep(mu_group_1, n_samp) mu[groups == levels(groups)[2]] &lt;- mu_group_2 bias &lt;- log_geo_mean - mu dat.normed &lt;- comp_table - matrix(rep(bias, n_taxa), nrow = n_taxa, byrow = T) colnames(dat.normed) &lt;- colnames(raw) # Convert to phyloseq otu &lt;- otu_table(dat.normed, taxa_are_rows = T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- exp(bias) tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_ancom2 &lt;- phyloseq(otu, sam, tax, phy) return(ps_ancom2) } k_ancom2 &lt;- norm_ANCOM2(k_raw, &quot;DIAGNOSIS&quot;) k_ancom2 7.2 ANCOM BC ANCOM BC (Lin and Peddada 2020) includes a normalization method that accounts for zero inflation and compositionality. This method corrects bias by estimating a sampling fraction, which is…. It accounts for sampling fraction by introducing a sample-specific offset in a linear regression framework, estimated form observed data. The offset term is the bias correction, and the linear regression in log scale is analogous to the log ratio transformation to deal with compositionality. This method also identifies and accounts for outlier zeros, structural zeros, and sampling zeros, using the same methodology as ANCOM2. library(ANCOMBC) norm_ANCOMBC &lt;- function(ps, group_var) { out = ancombc(ps, formula = group_var, group = group_var, struc_zero = T, neg_lb = T, zero_cut = .9) #it is recommended to set neg_lb = TRUE when the sample size per group is relatively large (e.g. &gt; 30) sampling.fraction &lt;- exp(out$samp_frac) # Normalize counts # feature_table &lt;- otu_table(ps, taxa_are_rows = T) feature_table &lt;- out$feature_table # otu_norm &lt;- t(t(otu) / sampling.fraction) otu_norm &lt;- t(t(feature_table) / sampling.fraction) otu &lt;- otu_table(otu_norm, T) sam &lt;- access(ps, &quot;sam_data&quot;) sam$scaling_factor &lt;- sampling.fraction tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_ancombc &lt;- phyloseq(otu, sam, tax, phy) return(ps_ancombc) } k_ancombc &lt;- norm_ANCOMBC(k_raw, group_var = &quot;DIAGNOSIS&quot;) ## Warning in data_prep(phyloseq, group, zero_cut, lib_cut, global = global): ## The multi-group comparison will be deactivated as the group variable has &lt; 3 ## categories. 7.3 ALDEx2 ALDEx2 (Fernandes et al. 2014) is a method for differential abundance analysis developed for use with microbiome or any other sequencing setting. It uses a clr log-ratio transformation instead of library size normalization. To deal with sparsity, it creates randomized instances via Monte Carlo draws based on Dirichlet-Multinomial distribution with probabilities from the raw count data, These are zero-free, so log-ratio transformations are possible. The idea is to estimate biological and sampling variation to calculate expected false discovery rate given variation. The method estimates technical variation within each sample using MC draws from Dirichlet distribution. For differential abundance, statistical tests are performed on each instance, p-values averaged, then corrected for multiple comparisons This method has been shown to be conservative compared to many of the library size normalization methods. library(ALDEx2) norm_ALDEx2 &lt;- function(ps, group_var) { raw &lt;- as.data.frame(otu_table(ps)) groups &lt;- get_variable(ps, group_var) # Run ALDEx2 function aldex.clr.res &lt;- aldex.clr( reads = raw, conds = groups, mc.samples = 128 ) # Extract each clr MC draw mc_draws &lt;- getMonteCarloInstances(aldex.clr.res) # Average each draw for the &#39;normalized&#39; clr read table dat.normed &lt;- matrix(0, nrow = nrow(mc_draws[[1]]) , ncol = length(mc_draws)) for (col in 1:ncol(dat.normed)) { dat.normed[, col] &lt;- apply(mc_draws[[col]], 1, mean) } rownames(dat.normed) &lt;- rownames(mc_draws[[1]]) colnames(dat.normed) &lt;- names(mc_draws) otu &lt;- otu_table(dat.normed, T) sam &lt;- access(ps, &quot;sam_data&quot;) tax &lt;- access(ps, &quot;tax_table&quot;) phy &lt;- access(ps, &quot;phy_tree&quot;) ps_aldex2 &lt;- phyloseq(otu, sam, tax, phy) return(ps_aldex2) } k_aldex2 &lt;- norm_ALDEx2(k_raw, group_var = &quot;DIAGNOSIS&quot;) ## operating in serial mode ## computing center with all features References "],["comparisions.html", "Chapter 8 Comparisions", " Chapter 8 Comparisions See all distance comparisions for Global patterns data library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 data.frame(raw = as.numeric(phyloseq::distance(gp_raw, &quot;bray&quot;)), tss = as.numeric(phyloseq::distance(gp_tss, &quot;bray&quot;)), css = as.numeric(phyloseq::distance(gp_css, &quot;bray&quot;)), deseq_rle = as.numeric(phyloseq::distance(gp_deseq_rle, &quot;bray&quot;)), deseq_vs = as.numeric(phyloseq::distance(gp_deseq_vs, &quot;bray&quot;)), rare = as.numeric(phyloseq::distance(gp_rare, &quot;bray&quot;)), tmm = as.numeric(phyloseq::distance(gp_tmm, &quot;bray&quot;)), diff = as.numeric(dist(get_variable(gp_raw, &quot;depth&quot;)))) %&gt;% ggpairs(columns = 1:7, upper = &quot;blank&quot;, diag = &quot;blank&quot;,ggplot2::aes(colour=diff)) See all distance comparisons for Kostic data data.frame(raw = as.numeric(phyloseq::distance(k_raw, &quot;bray&quot;)), tss = as.numeric(phyloseq::distance(k_tss, &quot;bray&quot;)), css = as.numeric(phyloseq::distance(k_css, &quot;bray&quot;)), deseq_rle = as.numeric(phyloseq::distance(k_deseq_rle, &quot;bray&quot;)), deseq_vs = as.numeric(phyloseq::distance(k_deseq_vs, &quot;bray&quot;)), rare = as.numeric(phyloseq::distance(k_rare, &quot;bray&quot;)), tmm = as.numeric(phyloseq::distance(k_tmm, &quot;bray&quot;)), diff = as.numeric(dist(get_variable(k_raw, &quot;depth&quot;)))) %&gt;% ggpairs(columns = 1:7, upper = &quot;blank&quot;, diag = &quot;blank&quot;,ggplot2::aes(colour=diff)) "],["references.html", "References", " References "]]
